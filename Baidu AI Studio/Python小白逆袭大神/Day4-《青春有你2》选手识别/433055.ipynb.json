{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# PaddleHub之《青春有你2》作业：五人识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 一、任务简介\n",
    "\n",
    "图像分类是计算机视觉的重要领域，它的目标是将图像分类到预定义的标签。近期，许多研究者提出很多不同种类的神经网络，并且极大的提升了分类算法的性能。本文以自己创建的数据集：青春有你2中选手识别为例子，介绍如何使用PaddleHub进行图像分类任务。\n",
    "\n",
    "<div  align=\"center\">   \n",
    "<img src=\"https://ai-studio-static-online.cdn.bcebos.com/dbaf5ba718f749f0836c342fa67f6d7954cb89f3796b4c8b981a03a1635f2fbe\" width = \"350\" height = \"250\" align=center />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CPU_NUM=1\n"
     ]
    }
   ],
   "source": [
    "#CPU环境启动请务必执行该指令\n",
    "%set_env CPU_NUM=1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting paddlehub==1.6.0\n",
      "\u001b[?25l  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/7f/9f/6617c2b8e9c5d847803ae89924b58bccd1b8fb2c98aa00e16531540591f2/paddlehub-1.6.0-py3-none-any.whl (206kB)\n",
      "\u001b[K     |████████████████████████████████| 215kB 10.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: flask>=1.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.6.0) (1.1.1)\n",
      "Requirement already satisfied: gunicorn>=19.10.0; sys_platform != \"win32\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.6.0) (20.0.4)\n",
      "Requirement already satisfied: numpy; python_version >= \"3\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.6.0) (1.16.4)\n",
      "Requirement already satisfied: pyyaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.6.0) (5.1.2)\n",
      "Requirement already satisfied: cma==2.7.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.6.0) (2.7.0)\n",
      "Requirement already satisfied: chardet==3.0.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.6.0) (3.0.4)\n",
      "Requirement already satisfied: sentencepiece in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.6.0) (0.1.85)\n",
      "Requirement already satisfied: pre-commit in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.6.0) (1.21.0)\n",
      "Requirement already satisfied: tensorboard>=1.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.6.0) (2.1.0)\n",
      "Requirement already satisfied: nltk in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.6.0) (3.4.5)\n",
      "Requirement already satisfied: requests in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.6.0) (2.22.0)\n",
      "Requirement already satisfied: colorlog in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.6.0) (4.1.0)\n",
      "Requirement already satisfied: pandas; python_version >= \"3\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.6.0) (0.23.4)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.6.0) (3.10.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.6.0) (1.12.0)\n",
      "Requirement already satisfied: opencv-python in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.6.0) (4.1.1.26)\n",
      "Requirement already satisfied: Pillow in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.6.0) (6.2.0)\n",
      "Requirement already satisfied: flake8 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.6.0) (3.7.9)\n",
      "Requirement already satisfied: tb-paddle in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.6.0) (0.3.6)\n",
      "Requirement already satisfied: yapf==0.26.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from paddlehub==1.6.0) (0.26.0)\n",
      "Requirement already satisfied: itsdangerous>=0.24 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.0->paddlehub==1.6.0) (1.1.0)\n",
      "Requirement already satisfied: Werkzeug>=0.15 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.0->paddlehub==1.6.0) (0.16.0)\n",
      "Requirement already satisfied: Jinja2>=2.10.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.0->paddlehub==1.6.0) (2.10.3)\n",
      "Requirement already satisfied: click>=5.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flask>=1.1.0->paddlehub==1.6.0) (7.0)\n",
      "Requirement already satisfied: setuptools>=3.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from gunicorn>=19.10.0; sys_platform != \"win32\"->paddlehub==1.6.0) (41.4.0)\n",
      "Requirement already satisfied: cfgv>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlehub==1.6.0) (2.0.1)\n",
      "Requirement already satisfied: aspy.yaml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlehub==1.6.0) (1.3.0)\n",
      "Requirement already satisfied: nodeenv>=0.11.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlehub==1.6.0) (1.3.4)\n",
      "Requirement already satisfied: identify>=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlehub==1.6.0) (1.4.10)\n",
      "Requirement already satisfied: virtualenv>=15.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlehub==1.6.0) (16.7.9)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlehub==1.6.0) (0.23)\n",
      "Requirement already satisfied: toml in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pre-commit->paddlehub==1.6.0) (0.10.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tensorboard>=1.15->paddlehub==1.6.0) (1.26.0)\n",
      "Requirement already satisfied: absl-py>=0.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tensorboard>=1.15->paddlehub==1.6.0) (0.8.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tensorboard>=1.15->paddlehub==1.6.0) (3.1.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tensorboard>=1.15->paddlehub==1.6.0) (0.4.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tensorboard>=1.15->paddlehub==1.6.0) (1.10.0)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tensorboard>=1.15->paddlehub==1.6.0) (0.33.6)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->paddlehub==1.6.0) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->paddlehub==1.6.0) (1.25.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests->paddlehub==1.6.0) (2019.9.11)\n",
      "Requirement already satisfied: python-dateutil>=2.5.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pandas; python_version >= \"3\"->paddlehub==1.6.0) (2.8.0)\n",
      "Requirement already satisfied: pytz>=2011k in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from pandas; python_version >= \"3\"->paddlehub==1.6.0) (2019.3)\n",
      "Requirement already satisfied: mccabe<0.7.0,>=0.6.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8->paddlehub==1.6.0) (0.6.1)\n",
      "Requirement already satisfied: entrypoints<0.4.0,>=0.3.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8->paddlehub==1.6.0) (0.3)\n",
      "Requirement already satisfied: pyflakes<2.2.0,>=2.1.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8->paddlehub==1.6.0) (2.1.1)\n",
      "Requirement already satisfied: pycodestyle<2.6.0,>=2.5.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from flake8->paddlehub==1.6.0) (2.5.0)\n",
      "Requirement already satisfied: moviepy in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from tb-paddle->paddlehub==1.6.0) (1.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from Jinja2>=2.10.1->flask>=1.1.0->paddlehub==1.6.0) (1.1.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from importlib-metadata; python_version < \"3.8\"->pre-commit->paddlehub==1.6.0) (0.6.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->paddlehub==1.6.0) (1.3.0)\n",
      "Requirement already satisfied: rsa<4.1,>=3.1.4 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->paddlehub==1.6.0) (4.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->paddlehub==1.6.0) (0.2.7)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard>=1.15->paddlehub==1.6.0) (4.0.0)\n",
      "Requirement already satisfied: imageio-ffmpeg>=0.2.0; python_version >= \"3.4\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from moviepy->tb-paddle->paddlehub==1.6.0) (0.3.0)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5; python_version >= \"3.4\" in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from moviepy->tb-paddle->paddlehub==1.6.0) (2.6.1)\n",
      "Requirement already satisfied: proglog<=1.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from moviepy->tb-paddle->paddlehub==1.6.0) (0.1.9)\n",
      "Requirement already satisfied: decorator<5.0,>=4.0.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from moviepy->tb-paddle->paddlehub==1.6.0) (4.4.0)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from moviepy->tb-paddle->paddlehub==1.6.0) (4.36.1)\n",
      "Requirement already satisfied: more-itertools in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from zipp>=0.5->importlib-metadata; python_version < \"3.8\"->pre-commit->paddlehub==1.6.0) (7.2.0)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.15->paddlehub==1.6.0) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in /opt/conda/envs/python35-paddle120-env/lib/python3.7/site-packages (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard>=1.15->paddlehub==1.6.0) (0.4.8)\n",
      "Installing collected packages: paddlehub\n",
      "  Found existing installation: paddlehub 1.5.0\n",
      "    Uninstalling paddlehub-1.5.0:\n",
      "      Successfully uninstalled paddlehub-1.5.0\n",
      "Successfully installed paddlehub-1.6.0\n"
     ]
    }
   ],
   "source": [
    "#安装paddlehub\n",
    "!pip install paddlehub==1.6.0 -i https://pypi.tuna.tsinghua.edu.cn/simple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## 二、任务实践\n",
    "### Step1、基础工作\n",
    "\n",
    "加载数据文件\n",
    "\n",
    "导入python包"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unzip:  cannot find or open file.zip, file.zip.zip or file.zip.ZIP.\r\n"
     ]
    }
   ],
   "source": [
    "# !unzip -o file.zip -d ./dataset/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import paddlehub as hub\n",
    "import requests\n",
    "import re\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step2、加载预训练模型\n",
    "\n",
    "接下来我们要在PaddleHub中选择合适的预训练模型来Finetune，由于是图像分类任务，因此我们使用经典的ResNet-50作为预训练模型。PaddleHub提供了丰富的图像分类预训练模型，包括了最新的神经网络架构搜索类的PNASNet，我们推荐您尝试不同的预训练模型来获得更好的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-04-30 13:57:12,018] [    INFO] - Installing resnet_v2_50_imagenet module\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading resnet_v2_50_imagenet\n",
      "[==================================================] 100.00%\n",
      "Uncompress /home/aistudio/.paddlehub/tmp/tmp3rxtshns/resnet_v2_50_imagenet\n",
      "[==================================================] 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-04-30 13:57:23,421] [    INFO] - Successfully installed resnet_v2_50_imagenet-1.0.1\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "module = hub.Module(name=\"resnet_v2_50_imagenet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step3、数据准备\n",
    "\n",
    "接着需要加载图片数据集。我们使用自定义的数据进行体验，请查看[适配自定义数据](https://github.com/PaddlePaddle/PaddleHub/wiki/PaddleHub适配自定义数据完成FineTune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['虞书欣', '许佳琪', '赵小棠', '安崎', '王承渲']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 获取标签\n",
    "with open('dataset/label_list.txt', 'r') as f:\n",
    "    label_list = []\n",
    "    while True:\n",
    "        text = f.readline()\n",
    "        if text:\n",
    "            label_list.append(text.replace('\\n', ''))\n",
    "        else:\n",
    "            break\n",
    "label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功下载了王承渲的第288张照片，地址：http://wx2.sinaimg.cn/orj360/006Plb3Wgy1ge6mxfkebcg30ig0ige82.gif\r"
     ]
    }
   ],
   "source": [
    "# 从百度图片采集图片\n",
    "headers = { \n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.99 Safari/537.36'\n",
    "    }\n",
    "\n",
    "for girlName in label_list:\n",
    "    pn = 0\n",
    "    pic_list = []\n",
    "    pic_list_temp = []\n",
    "    while len(pic_list) < 300:\n",
    "        url = 'http://image.baidu.com/search/flip?tn=baiduimage&ie=utf-8&word=' + girlName + '青春有你 高清' + '&pn=' + str(pn)\n",
    "        response = requests.get(url,headers=headers)\n",
    "        pic_list_temp = re.findall('\"objURL\":\"(.*?)\"', response.text)\n",
    "        pic_list.extend(pic_list_temp)\n",
    "        pn += 60\n",
    "    print('找到了' + str(len(pic_list)) + '张' + girlName + '的照片')\n",
    "    \n",
    "    # 为训练集、测试集和验证集各创建一个文件夹，其中再为每位小姐姐各创建一个文件夹。\n",
    "    training_dataset_path = 'dataset/training_dataset/' + girlName + '/'\n",
    "    if not os.path.exists(training_dataset_path):\n",
    "        os.makedirs(training_dataset_path)\n",
    "    # test_dataset_path = 'dataset/test_dataset/' + girlName + '/'\n",
    "    # if not os.path.exists(test_dataset_path):\n",
    "    #     os.makedirs(test_dataset_path)\n",
    "    validate_dataset_path = 'dataset/validate_dataset/' + girlName + '/'\n",
    "    if not os.path.exists(validate_dataset_path):\n",
    "        os.makedirs(validate_dataset_path)\n",
    "    \n",
    "    num = 1\n",
    "    for i,pic_url in enumerate(pic_list):\n",
    "        try:\n",
    "            pic = requests.get(pic_url, timeout = 3)\n",
    "            pic_name = str(num) + '.jpg'\n",
    "            if num<=240:\n",
    "                with open(training_dataset_path + pic_name, 'wb') as f:\n",
    "                    f.write(pic.content)\n",
    "            # elif num>240 and num<=270:\n",
    "            #     with open(test_dataset_path + pic_name, 'wb') as f:\n",
    "            #         f.write(pic.content)\n",
    "            else:\n",
    "                with open(validate_dataset_path + pic_name, 'wb') as f:\n",
    "                    f.write(pic.content)\n",
    "            print('成功下载了' + girlName + '的第%s张照片，地址：%s' % (str(num), str(pic_url)))\n",
    "            num += 1\n",
    "\n",
    "        except Exception as e:\n",
    "            print('下载' + girlName + '的第%s张照片时失败，地址：%s' % (str(num), str(pic_url)))\n",
    "            print(e)\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 更新数据集文件\n",
    "with open('dataset/train_list.txt', 'w') as f:\n",
    "    for (dirpath,dirnames,filenames) in os.walk('dataset/training_dataset'):\n",
    "        for filename in filenames:\n",
    "            filePath = os.path.join(dirpath,filename).replace('dataset/t', 't')\n",
    "            label = str(label_list.index(dirpath.split('/')[2]))\n",
    "            f.write(filePath + ' ' + label + '\\n')\n",
    "\n",
    "# with open('dataset/test_list.txt', 'w') as f:\n",
    "#     for (dirpath,dirnames,filenames) in os.walk('dataset/test_dataset'):\n",
    "#         for filename in filenames:\n",
    "#             filePath = os.path.join(dirpath,filename).replace('dataset/t', 't')\n",
    "#             label = str(label_list.index(dirpath.split('/')[2]))\n",
    "#             f.write(filePath + ' ' + label + '\\n')\n",
    "\n",
    "with open('dataset/validate_list.txt', 'w') as f:\n",
    "    for (dirpath,dirnames,filenames) in os.walk('dataset/validate_dataset'):\n",
    "        for filename in filenames:\n",
    "            filePath = os.path.join(dirpath,filename).replace('dataset/v', 'v')\n",
    "            label = str(label_list.index(dirpath.split('/')[2]))\n",
    "            f.write(filePath + ' ' + label + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from paddlehub.dataset.base_cv_dataset import BaseCVDataset\n",
    "   \n",
    "class DemoDataset(BaseCVDataset):\t\n",
    "   def __init__(self):\t\n",
    "       # 数据集存放位置\n",
    "       \n",
    "       self.dataset_dir = \"dataset\"\n",
    "       super(DemoDataset, self).__init__(\n",
    "           base_path=self.dataset_dir,\n",
    "           train_list_file=\"train_list.txt\",\n",
    "           validate_list_file=\"validate_list.txt\",\n",
    "           test_list_file=\"test_list.txt\",\n",
    "           label_list_file=\"label_list.txt\",\n",
    "           )\n",
    "dataset = DemoDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step4、生成数据读取器\n",
    "\n",
    "接着生成一个图像分类的reader，reader负责将dataset的数据进行预处理，接着以特定格式组织并输入给模型进行训练。\n",
    "\n",
    "当我们生成一个图像分类的reader时，需要指定输入图片的大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-04-26 22:40:24,281] [    INFO] - Dataset label map = {'虞书欣': 0, '许佳琪': 1, '赵小棠': 2, '安崎': 3, '王承渲': 4}\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "data_reader = hub.reader.ImageClassificationReader(\n",
    "    image_width=module.get_expected_image_width(),\n",
    "    image_height=module.get_expected_image_height(),\n",
    "    images_mean=module.get_pretrained_images_mean(),\n",
    "    images_std=module.get_pretrained_images_std(),\n",
    "    dataset=dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step5、配置策略\n",
    "在进行Finetune前，我们可以设置一些运行时的配置，例如如下代码中的配置，表示：\n",
    "\n",
    "* `use_cuda`：设置为False表示使用CPU进行训练。如果您本机支持GPU，且安装的是GPU版本的PaddlePaddle，我们建议您将这个选项设置为True；\n",
    "\n",
    "* `epoch`：迭代轮数；\n",
    "\n",
    "* `batch_size`：每次训练的时候，给模型输入的每批数据大小为32，模型训练时能够并行处理批数据，因此batch_size越大，训练的效率越高，但是同时带来了内存的负荷，过大的batch_size可能导致内存不足而无法训练，因此选择一个合适的batch_size是很重要的一步；\n",
    "\n",
    "* `log_interval`：每隔10 step打印一次训练日志；\n",
    "\n",
    "* `eval_interval`：每隔50 step在验证集上进行一次性能评估；\n",
    "\n",
    "* `checkpoint_dir`：将训练的参数和数据保存到cv_finetune_turtorial_demo目录中；\n",
    "\n",
    "* `strategy`：使用DefaultFinetuneStrategy策略进行finetune；\n",
    "\n",
    "更多运行配置，请查看[RunConfig](https://github.com/PaddlePaddle/PaddleHub/wiki/PaddleHub-API:-RunConfig)\n",
    "\n",
    "同时PaddleHub提供了许多优化策略，如`AdamWeightDecayStrategy`、`ULMFiTStrategy`、`DefaultFinetuneStrategy`等，详细信息参见[策略](https://github.com/PaddlePaddle/PaddleHub/wiki/PaddleHub-API:-Strategy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-04-26 22:40:28,055] [    INFO] - Checkpoint dir: cv_finetune_turtorial_demo\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "config = hub.RunConfig(\n",
    "    use_cuda=True,                              #是否使用GPU训练，默认为False；\n",
    "    num_epoch=3,                                #Fine-tune的轮数；\n",
    "    checkpoint_dir=\"cv_finetune_turtorial_demo\",#模型checkpoint保存路径, 若用户没有指定，程序会自动生成；\n",
    "    batch_size=20,                              #训练的批大小，如果使用GPU，请根据实际情况调整batch_size；\n",
    "    eval_interval=10,                           #模型评估的间隔，默认每100个step评估一次验证集；\n",
    "    strategy=hub.finetune.strategy.DefaultFinetuneStrategy())  #Fine-tune优化策略；"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step6、组建Finetune Task\n",
    "有了合适的预训练模型和准备要迁移的数据集后，我们开始组建一个Task。\n",
    "\n",
    "由于该数据设置是一个二分类的任务，而我们下载的分类module是在ImageNet数据集上训练的千分类模型，所以我们需要对模型进行简单的微调，把模型改造为一个二分类模型：\n",
    "\n",
    "1. 获取module的上下文环境，包括输入和输出的变量，以及Paddle Program；\n",
    "2. 从输出变量中找到特征图提取层feature_map；\n",
    "3. 在feature_map后面接入一个全连接层，生成Task；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-04-26 22:40:31,304] [    INFO] - 267 pretrained paramaters loaded by PaddleHub\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "input_dict, output_dict, program = module.context(trainable=True)\n",
    "img = input_dict[\"image\"]\n",
    "feature_map = output_dict[\"feature_map\"]\n",
    "feed_list = [img.name]\n",
    "\n",
    "task = hub.ImageClassifierTask(\n",
    "    data_reader=data_reader,\n",
    "    feed_list=feed_list,\n",
    "    feature=feature_map,\n",
    "    num_classes=dataset.num_labels,\n",
    "    config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step5、开始Finetune\n",
    "\n",
    "我们选择`finetune_and_eval`接口来进行模型训练，这个接口在finetune的过程中，会周期性的进行模型效果的评估，以便我们了解整个训练过程的性能变化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-04-26 22:40:39,120] [    INFO] - Strategy with slanted triangle learning rate, L2 regularization, \u001b[0m\n",
      "\u001b[32m[2020-04-26 22:40:39,155] [    INFO] - Try loading checkpoint from cv_finetune_turtorial_demo/ckpt.meta\u001b[0m\n",
      "\u001b[32m[2020-04-26 22:40:39,156] [    INFO] - PaddleHub model checkpoint not found, start from scratch...\u001b[0m\n",
      "\u001b[32m[2020-04-26 22:40:39,211] [    INFO] - PaddleHub finetune start\u001b[0m\n",
      "\u001b[36m[2020-04-26 22:40:42,312] [   TRAIN] - step 10 / 180: loss=0.78811 acc=0.79500 [step/sec: 3.52]\u001b[0m\n",
      "\u001b[32m[2020-04-26 22:40:42,313] [    INFO] - Evaluation on dev dataset start\u001b[0m\n",
      "share_vars_from is set, scope is ignored.\n",
      "\u001b[34m[2020-04-26 22:40:47,000] [    EVAL] - [dev dataset evaluation result] loss=1.62775 acc=0.38504 [step/sec: 3.54]\u001b[0m\n",
      "\u001b[34m[2020-04-26 22:40:47,001] [    EVAL] - best model saved to cv_finetune_turtorial_demo/best_model [best acc=0.38504]\u001b[0m\n",
      "\u001b[36m[2020-04-26 22:40:49,058] [   TRAIN] - step 20 / 180: loss=0.27572 acc=0.94000 [step/sec: 12.97]\u001b[0m\n",
      "\u001b[32m[2020-04-26 22:40:49,059] [    INFO] - Evaluation on dev dataset start\u001b[0m\n",
      "\u001b[34m[2020-04-26 22:40:52,833] [    EVAL] - [dev dataset evaluation result] loss=1.96948 acc=0.36154 [step/sec: 3.69]\u001b[0m\n",
      "\u001b[36m[2020-04-26 22:40:53,586] [   TRAIN] - step 30 / 180: loss=0.37949 acc=0.88500 [step/sec: 13.32]\u001b[0m\n",
      "\u001b[32m[2020-04-26 22:40:53,587] [    INFO] - Evaluation on dev dataset start\u001b[0m\n",
      "\u001b[34m[2020-04-26 22:40:57,393] [    EVAL] - [dev dataset evaluation result] loss=2.12188 acc=0.39829 [step/sec: 3.65]\u001b[0m\n",
      "\u001b[34m[2020-04-26 22:40:57,394] [    EVAL] - best model saved to cv_finetune_turtorial_demo/best_model [best acc=0.39829]\u001b[0m\n",
      "\u001b[36m[2020-04-26 22:40:59,034] [   TRAIN] - step 40 / 180: loss=0.34142 acc=0.88000 [step/sec: 13.35]\u001b[0m\n",
      "\u001b[32m[2020-04-26 22:40:59,036] [    INFO] - Evaluation on dev dataset start\u001b[0m\n",
      "\u001b[34m[2020-04-26 22:41:02,699] [    EVAL] - [dev dataset evaluation result] loss=2.13460 acc=0.39060 [step/sec: 3.79]\u001b[0m\n",
      "\u001b[36m[2020-04-26 22:41:03,458] [   TRAIN] - step 50 / 180: loss=0.21681 acc=0.94500 [step/sec: 13.22]\u001b[0m\n",
      "\u001b[32m[2020-04-26 22:41:03,459] [    INFO] - Evaluation on dev dataset start\u001b[0m\n",
      "\u001b[34m[2020-04-26 22:41:07,193] [    EVAL] - [dev dataset evaluation result] loss=2.40729 acc=0.36838 [step/sec: 3.72]\u001b[0m\n",
      "\u001b[36m[2020-04-26 22:41:07,952] [   TRAIN] - step 60 / 180: loss=0.39372 acc=0.87500 [step/sec: 13.21]\u001b[0m\n",
      "\u001b[32m[2020-04-26 22:41:07,953] [    INFO] - Evaluation on dev dataset start\u001b[0m\n",
      "\u001b[34m[2020-04-26 22:41:11,653] [    EVAL] - [dev dataset evaluation result] loss=2.92772 acc=0.34103 [step/sec: 3.75]\u001b[0m\n",
      "\u001b[36m[2020-04-26 22:41:14,701] [   TRAIN] - step 70 / 180: loss=0.20207 acc=0.93000 [step/sec: 3.63]\u001b[0m\n",
      "\u001b[32m[2020-04-26 22:41:14,702] [    INFO] - Evaluation on dev dataset start\u001b[0m\n",
      "\u001b[34m[2020-04-26 22:41:18,505] [    EVAL] - [dev dataset evaluation result] loss=2.27091 acc=0.35598 [step/sec: 3.66]\u001b[0m\n",
      "\u001b[36m[2020-04-26 22:41:19,263] [   TRAIN] - step 80 / 180: loss=0.25087 acc=0.93000 [step/sec: 13.23]\u001b[0m\n",
      "\u001b[32m[2020-04-26 22:41:19,264] [    INFO] - Evaluation on dev dataset start\u001b[0m\n",
      "\u001b[34m[2020-04-26 22:41:23,009] [    EVAL] - [dev dataset evaluation result] loss=2.03049 acc=0.39915 [step/sec: 3.72]\u001b[0m\n",
      "\u001b[34m[2020-04-26 22:41:23,010] [    EVAL] - best model saved to cv_finetune_turtorial_demo/best_model [best acc=0.39915]\u001b[0m\n",
      "\u001b[36m[2020-04-26 22:41:24,649] [   TRAIN] - step 90 / 180: loss=0.24224 acc=0.91500 [step/sec: 13.26]\u001b[0m\n",
      "\u001b[32m[2020-04-26 22:41:24,651] [    INFO] - Evaluation on dev dataset start\u001b[0m\n",
      "\u001b[34m[2020-04-26 22:41:28,378] [    EVAL] - [dev dataset evaluation result] loss=2.20954 acc=0.43504 [step/sec: 3.73]\u001b[0m\n",
      "\u001b[34m[2020-04-26 22:41:28,379] [    EVAL] - best model saved to cv_finetune_turtorial_demo/best_model [best acc=0.43504]\u001b[0m\n",
      "\u001b[36m[2020-04-26 22:41:30,044] [   TRAIN] - step 100 / 180: loss=0.15973 acc=0.95500 [step/sec: 13.23]\u001b[0m\n",
      "\u001b[32m[2020-04-26 22:41:30,045] [    INFO] - Evaluation on dev dataset start\u001b[0m\n",
      "\u001b[34m[2020-04-26 22:41:33,659] [    EVAL] - [dev dataset evaluation result] loss=2.37806 acc=0.41154 [step/sec: 3.85]\u001b[0m\n",
      "\u001b[36m[2020-04-26 22:41:34,434] [   TRAIN] - step 110 / 180: loss=0.23663 acc=0.92500 [step/sec: 12.95]\u001b[0m\n",
      "\u001b[32m[2020-04-26 22:41:34,435] [    INFO] - Evaluation on dev dataset start\u001b[0m\n",
      "\u001b[34m[2020-04-26 22:41:38,170] [    EVAL] - [dev dataset evaluation result] loss=2.25919 acc=0.40769 [step/sec: 3.72]\u001b[0m\n",
      "\u001b[36m[2020-04-26 22:41:38,923] [   TRAIN] - step 120 / 180: loss=0.21052 acc=0.94000 [step/sec: 13.31]\u001b[0m\n",
      "\u001b[32m[2020-04-26 22:41:38,924] [    INFO] - Evaluation on dev dataset start\u001b[0m\n",
      "\u001b[34m[2020-04-26 22:41:42,553] [    EVAL] - [dev dataset evaluation result] loss=2.13027 acc=0.38462 [step/sec: 3.83]\u001b[0m\n",
      "\u001b[36m[2020-04-26 22:41:45,837] [   TRAIN] - step 130 / 180: loss=0.11031 acc=0.96500 [step/sec: 3.33]\u001b[0m\n",
      "\u001b[32m[2020-04-26 22:41:45,838] [    INFO] - Evaluation on dev dataset start\u001b[0m\n",
      "\u001b[34m[2020-04-26 22:41:49,756] [    EVAL] - [dev dataset evaluation result] loss=2.42932 acc=0.35812 [step/sec: 3.55]\u001b[0m\n",
      "\u001b[36m[2020-04-26 22:41:50,512] [   TRAIN] - step 140 / 180: loss=0.09931 acc=0.97000 [step/sec: 13.28]\u001b[0m\n",
      "\u001b[32m[2020-04-26 22:41:50,513] [    INFO] - Evaluation on dev dataset start\u001b[0m\n",
      "\u001b[34m[2020-04-26 22:41:54,251] [    EVAL] - [dev dataset evaluation result] loss=2.38016 acc=0.35684 [step/sec: 3.72]\u001b[0m\n",
      "\u001b[36m[2020-04-26 22:41:55,007] [   TRAIN] - step 150 / 180: loss=0.13607 acc=0.95500 [step/sec: 13.27]\u001b[0m\n",
      "\u001b[32m[2020-04-26 22:41:55,008] [    INFO] - Evaluation on dev dataset start\u001b[0m\n",
      "\u001b[34m[2020-04-26 22:41:58,839] [    EVAL] - [dev dataset evaluation result] loss=2.35142 acc=0.37607 [step/sec: 3.63]\u001b[0m\n",
      "\u001b[36m[2020-04-26 22:41:59,594] [   TRAIN] - step 160 / 180: loss=0.12342 acc=0.97000 [step/sec: 13.29]\u001b[0m\n",
      "\u001b[32m[2020-04-26 22:41:59,595] [    INFO] - Evaluation on dev dataset start\u001b[0m\n",
      "\u001b[34m[2020-04-26 22:42:03,240] [    EVAL] - [dev dataset evaluation result] loss=2.31227 acc=0.40769 [step/sec: 3.82]\u001b[0m\n",
      "\u001b[36m[2020-04-26 22:42:03,996] [   TRAIN] - step 170 / 180: loss=0.16030 acc=0.94000 [step/sec: 13.25]\u001b[0m\n",
      "\u001b[32m[2020-04-26 22:42:03,997] [    INFO] - Evaluation on dev dataset start\u001b[0m\n",
      "\u001b[34m[2020-04-26 22:42:07,626] [    EVAL] - [dev dataset evaluation result] loss=2.41350 acc=0.40427 [step/sec: 3.84]\u001b[0m\n",
      "\u001b[36m[2020-04-26 22:42:08,386] [   TRAIN] - step 180 / 180: loss=0.34517 acc=0.88000 [step/sec: 13.19]\u001b[0m\n",
      "\u001b[32m[2020-04-26 22:42:08,387] [    INFO] - Evaluation on dev dataset start\u001b[0m\n",
      "\u001b[34m[2020-04-26 22:42:12,114] [    EVAL] - [dev dataset evaluation result] loss=2.25652 acc=0.36966 [step/sec: 3.73]\u001b[0m\n",
      "\u001b[32m[2020-04-26 22:42:12,116] [    INFO] - Evaluation on dev dataset start\u001b[0m\n",
      "\u001b[34m[2020-04-26 22:42:15,784] [    EVAL] - [dev dataset evaluation result] loss=2.25652 acc=0.36966 [step/sec: 3.79]\u001b[0m\n",
      "\u001b[32m[2020-04-26 22:42:15,785] [    INFO] - Load the best model from cv_finetune_turtorial_demo/best_model\u001b[0m\n",
      "\u001b[32m[2020-04-26 22:42:16,046] [    INFO] - Evaluation on test dataset start\u001b[0m\n",
      "\u001b[34m[2020-04-26 22:42:16,317] [    EVAL] - [test dataset evaluation result] loss=0.11739 acc=1.00000 [step/sec: 86.59]\u001b[0m\n",
      "\u001b[32m[2020-04-26 22:42:16,318] [    INFO] - Saving model checkpoint to cv_finetune_turtorial_demo/step_180\u001b[0m\n",
      "\u001b[32m[2020-04-26 22:42:17,458] [    INFO] - PaddleHub finetune finished.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "run_states = task.finetune_and_eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Step6、预测\n",
    "\n",
    "当Finetune完成后，我们使用模型来进行预测，先通过以下命令来获取测试的图片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2020-04-26 22:56:41,642] [    INFO] - PaddleHub predict start\u001b[0m\n",
      "\u001b[32m[2020-04-26 22:56:41,643] [    INFO] - The best model has been loaded\u001b[0m\n",
      "share_vars_from is set, scope is ignored.\n",
      "\u001b[32m[2020-04-26 22:56:41,964] [    INFO] - PaddleHub predict finished.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[8.1702286e-01, 5.7456512e-02, 1.1043123e-01, 8.5629504e-03,\n",
      "        6.5265461e-03],\n",
      "       [1.2919698e-02, 8.8055885e-01, 3.7920827e-03, 8.3640136e-02,\n",
      "        1.9089244e-02],\n",
      "       [1.4890515e-02, 1.9226942e-02, 9.5445740e-01, 3.1291794e-03,\n",
      "        8.2958937e-03],\n",
      "       [8.7071682e-04, 1.7553908e-01, 2.6798041e-03, 8.1298715e-01,\n",
      "        7.9232324e-03],\n",
      "       [8.6215365e-04, 1.0457500e-03, 2.7515253e-04, 1.8380836e-03,\n",
      "        9.9597883e-01]], dtype=float32)]\n",
      "[0 1 2 3 4]\n",
      "input 1 is dataset/test/yushuxin.jpg, and the predict result is 虞书欣\n",
      "input 2 is dataset/test/xujiaqi.jpg, and the predict result is 许佳琪\n",
      "input 3 is dataset/test/zhaoxiaotang.jpg, and the predict result is 赵小棠\n",
      "input 4 is dataset/test/anqi.jpg, and the predict result is 安崎\n",
      "input 5 is dataset/test/wangchengxuan.jpg, and the predict result is 王承渲\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "with open(\"dataset/test_list.txt\",\"r\") as f:\n",
    "    filepath = f.readlines()\n",
    "\n",
    "data = ['dataset/' + filepath[0].split(\" \")[0],'dataset/' + filepath[1].split(\" \")[0],'dataset/' + filepath[2].split(\" \")[0],'dataset/' + filepath[3].split(\" \")[0],'dataset/' + filepath[4].split(\" \")[0]]\n",
    "\n",
    "\n",
    "label_map = dataset.label_dict()\n",
    "index = 0\n",
    "run_states = task.predict(data=data)\n",
    "results = [run_state.run_results for run_state in run_states]\n",
    "\n",
    "for batch_result in results:\n",
    "    print(batch_result)\n",
    "    batch_result = np.argmax(batch_result, axis=2)[0]\n",
    "    print(batch_result)\n",
    "    for result in batch_result:\n",
    "        index += 1\n",
    "        result = label_map[result]\n",
    "        print(\"input %i is %s, and the predict result is %s\" %\n",
    "              (index, data[index - 1], result))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PaddlePaddle 1.7.1 (Python 3.5)",
   "language": "python",
   "name": "py35-paddle1.2.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
